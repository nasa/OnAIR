# GSC-19165-1, "The On-Board Artificial Intelligence Research (OnAIR) Platform"
#
# Copyright Â© 2023 United States Government as represented by the Administrator of
# the National Aeronautics and Space Administration. No copyright is claimed in the
# United States under Title 17, U.S. Code. All Other Rights Reserved.
#
# Licensed under the NASA Open Source Agreement version 1.3
# See "NOSA GSC-19165-1 OnAIR.pdf"

import simdkalman
import numpy as np
from onair.src.ai_components.ai_plugin_abstract.ai_plugin import AIPlugin


class Plugin(AIPlugin):
    def __init__(self, name, headers, window_size=3):
        """
        :param headers: (int) length of time agent examines
        :param window_size: (int) size of time window to examine
        """
        super().__init__(name, headers)
        self.frames = []
        self.component_name = name
        self.headers = headers
        self.window_size = window_size

        self.kf = simdkalman.KalmanFilter(
            state_transition=[[1, 1], [0, 1]],  # matrix A
            process_noise=np.diag([0.1, 0.01]),  # Q
            observation_model=np.array([[1, 0]]),  # H
            observation_noise=1.0,
        )  # R

    #### START: Classes mandated by plugin architecture
    def update(self, frame):
        """
        :param frame: (list of floats) input sequence of len (input_dim)
        :return: None
        """
        for data_point_index in range(len(frame)):
            if len(self.frames) < len(
                frame
            ):  # If the frames variable is empty, append each data point in frame to it, each point wrapped as a list
                # This is done so the data can have each attribute grouped in one list before being passed to kalman
                # Ex: [[1:00, 1:01, 1:02, 1:03, 1:04, 1:05], [1, 2, 3, 4, 5]]
                self.frames.append([frame[data_point_index]])
            else:
                self.frames[data_point_index].append(frame[data_point_index])
                if (
                    len(self.frames[data_point_index]) > self.window_size
                ):  # If after adding a point to the frame, that attribute is larger than the window_size, take out the first element
                    self.frames[data_point_index].pop(0)

    def render_reasoning(self):
        """
        System should return its diagnosis
        """
        broken_attributes = self.frame_diagnosis(self.frames, self.headers)
        return broken_attributes

    #### END: Classes mandated by plugin architecture

    # Gets mean of values
    def mean(self, values):
        return sum(values) / len(values)

    # Gets absolute value residual from actual and predicted value
    def residual(self, predicted, actual):
        return abs(float(actual) - float(predicted))

    # Gets standard deviation of data
    def std_dev(self, data):
        return np.std(data)

    # Takes in the kf being used, the data, how many prediction "steps" it will make, and an optional initial value
    # Gives a prediction values based on given parameters
    def predict(self, data, forward_steps, inital_val=None):
        for i in range(len(data)):
            data[i] = float(
                data[i]
            )  # Makes sure all the data being worked with is a float
        if inital_val != None:
            smoothed = self.kf.smooth(
                data, initial_value=[float(inital_val), 0]
            )  # If there's an initial value, smooth it along that value
        else:
            smoothed = self.kf.smooth(data)  # If not, smooth it however you like
        predicted = self.kf.predict(
            data, forward_steps
        )  # Make a prediction on the smoothed data
        return predicted

    def predictions_for_given_data(self, data):
        returned_data = []
        initial_val = data[0]
        for item in range(len(data) - 1):
            predicted = self.predict(data[0 : item + 1], 1, initial_val)
            actual_next_state = data[item + 1]
            pred_mean = predicted.observations.mean
            returned_data.append(pred_mean)
        if len(returned_data) == 0:  # If there's not enough data just set it to 0
            returned_data.append(0)
        return returned_data

    # Get data, make predictions, and then find the errors for these predictions
    def generate_residuals_for_given_data(self, data):
        residuals = []
        initial_val = data[0]
        for item in range(len(data) - 1):
            predicted = self.predict(data[0 : item + 1], 1, initial_val)
            actual_next_state = data[item + 1]
            pred_mean = predicted.observations.mean
            residual_error = float(self.residual(pred_mean, actual_next_state))
            residuals.append(residual_error)
        if (
            len(residuals) == 0
        ):  # If there are no residuals because the data is of length 1, then just say residuals is equal to [0]
            residuals.append(0)
        return residuals

    # Info: takes a chunk of data of n size. Walks through it and gets residual errors.
    # Takes the mean of the errors and determines if they're too large overall in order to determine whether or not there's a chunk in said error.
    def current_attribute_chunk_get_error(self, data):
        residuals = self.generate_residuals_for_given_data(data)
        mean_residuals = abs(self.mean(residuals))
        if abs(mean_residuals) < 1.5:
            return False
        return True

    def frame_diagnosis(self, frame, headers):
        kal_broken_attributes = []
        for attribute_index in range(len(frame)):
            error = self.current_attribute_chunk_get_error(frame[attribute_index])
            if error and not headers[attribute_index].upper() == "TIME":
                kal_broken_attributes.append(headers[attribute_index])
        return kal_broken_attributes
